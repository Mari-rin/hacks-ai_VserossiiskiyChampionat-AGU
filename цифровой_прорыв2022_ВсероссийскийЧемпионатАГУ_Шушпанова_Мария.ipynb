{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "permanent-forge",
   "metadata": {},
   "source": [
    "# Цифровой Прорыв 2022, Всероссийский Чемпионат, задача от АГУ Collector\n",
    "\n",
    "Выполнила Шушпанова Мария.\n",
    "\n",
    "Загрузим необходимые для работы библиотеки, инициализируем датасеты для обучения и теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formed-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, BertConfig, DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertModel\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split, SequentialSampler\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lbm\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designed-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_issues = pd.read_csv('train_issues.csv')\n",
    "train_comments = pd.read_csv('train_comments.csv')\n",
    "employees = pd.read_csv('employees.csv')\n",
    "test_issues = pd.read_csv('test_issues.csv')\n",
    "test_comments = pd.read_csv('test_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-cursor",
   "metadata": {},
   "source": [
    "Посмотрим на информацию о таблице с характеристиками работников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "residential-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343 entries, 0 to 342\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   id                          343 non-null    int64 \n",
      " 1   active                      343 non-null    int64 \n",
      " 2   full_name                   343 non-null    object\n",
      " 3   position                    193 non-null    object\n",
      " 4   hiring_type                 260 non-null    object\n",
      " 5   payment_type                221 non-null    object\n",
      " 6   salary_calculation_type     33 non-null     object\n",
      " 7   english_level               16 non-null     object\n",
      " 8   passport                    343 non-null    int64 \n",
      " 9   is_nda_signed               343 non-null    int64 \n",
      " 10  is_labor_contract_signed    343 non-null    int64 \n",
      " 11  is_added_to_internal_chats  343 non-null    int64 \n",
      " 12  is_added_one_to_one         343 non-null    int64 \n",
      "dtypes: int64(7), object(6)\n",
      "memory usage: 35.0+ KB\n"
     ]
    }
   ],
   "source": [
    "employees.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-newfoundland",
   "metadata": {},
   "source": [
    "Так как в некоторых столбцах много пропусков, то мы не будем использовать эти столбцы в дальнейшем, так как информация в них не релевантна.\n",
    "Преобразуем в числовой вид необходимые столбцы и удалим ненужные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charitable-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees['position'] = employees['position'].astype(str)\n",
    "employees['position'] = employees['position'].apply(lambda x: x.replace('nan', '-1'))\n",
    "\n",
    "spaces_sub = re.compile(\"\\s+\")\n",
    "employees['position'] = employees['position'].apply(lambda x: spaces_sub.sub(\"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daily-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_int = list(range(employees['position'].nunique()))\n",
    "group_str = list(set(list(employees['position'].unique())))\n",
    "position_group = dict(zip(group_str, group_int))\n",
    "position_group ['-1'] = '-1'\n",
    "employees['position'] = employees['position'].map(position_group)\n",
    "employees['position'] = employees['position'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "three-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees['hiring_type'] = employees['hiring_type'].astype(str)\n",
    "employees['hiring_type'] = employees['hiring_type'].apply(lambda x: x.replace('nan', '-1'))\n",
    "\n",
    "hiring_type_str = list(set(list(employees['hiring_type'].unique())))\n",
    "hiring_type_int = list(range(employees['hiring_type'].nunique()))\n",
    "hiring_type_group = dict(zip(hiring_type_str, hiring_type_int))\n",
    "hiring_type_group ['-1'] = '-1'\n",
    "employees['hiring_type'] = employees['hiring_type'].map(hiring_type_group)\n",
    "employees['hiring_type'] = employees['hiring_type'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incorporated-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees['payment_type'] = employees['payment_type'].astype(str)\n",
    "employees['payment_type'] = employees['payment_type'].apply(lambda x: x.replace('nan', '-1'))\n",
    "\n",
    "payment_type_str = list(set(list(employees['payment_type'].unique())))\n",
    "payment_type_int = list(range(employees['payment_type'].nunique()))\n",
    "payment_type_group = dict(zip(payment_type_str, payment_type_int))\n",
    "payment_type_group ['-1'] = '-1'\n",
    "employees['payment_type'] = employees['payment_type'].map(payment_type_group)\n",
    "employees['payment_type'] = employees['payment_type'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dietary-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_new = employees.drop(['full_name', 'salary_calculation_type', 'english_level'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-effect",
   "metadata": {},
   "source": [
    "Создадим таблицы с теми, кто выполнял и кто ствил задачи, после объединим с основной таблицей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "embedded-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_assignee = employees_new.copy()\n",
    "employees_creator = employees_new.copy()\n",
    "\n",
    "employees_assignee = employees_assignee.rename(columns= {'id' : 'assignee_id', 'active' : 'assignee_active', \n",
    "                                                         'position' : 'assignee_position', 'hiring_type' : 'assignee_hiring_type',\n",
    "                                                        'payment_type' : 'assignee_payment_type', 'passport' : 'assignee_passport',\n",
    "                                                        'is_nda_signed' : 'assignee_is_nda_signed', 'is_labor_contract_signed' : 'assignee_is_labor_contract_signed',\n",
    "                                                        'is_added_to_internal_chats' : 'assignee_is_added_to_internal_chats', 'is_added_one_to_one' : 'assignee_is_added_one_to_one'})\n",
    "employees_creator = employees_creator.rename(columns= {'id' : 'creator_id', 'active' : 'creator_active', \n",
    "                                                         'position' : 'creator_position', 'hiring_type' : 'creator_hiring_type',\n",
    "                                                        'payment_type' : 'creator_payment_type', 'passport' : 'creator_passport',\n",
    "                                                        'is_nda_signed' : 'creatoris_nda_signed', 'is_labor_contract_signed' : 'creator_is_labor_contract_signed',\n",
    "                                                        'is_added_to_internal_chats' : 'creator_is_added_to_internal_chats', 'is_added_one_to_one' : 'creator_is_added_one_to_one'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "detected-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iss_empl_assignie = pd.merge(train_issues, employees_assignee, on = 'assignee_id', how = 'left')\n",
    "train_new = pd.merge(train_iss_empl_assignie, employees_creator, on = 'creator_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-relevance",
   "metadata": {},
   "source": [
    "Проведем T-тест, чтобы понять, какие столбцы влияют на время выполнения той или иной задачи. возьмем порог 99% доверия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "received-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_train = [x for x in train_new.columns if x not in ['id', 'assignee_id', 'creator_id',\n",
    "                                                       'key', 'created', 'project_id',\n",
    "                                                       'summary', 'overall_worklogs']]\n",
    "\n",
    "def ttest_all(column_name, alfa, target, list_name):\n",
    "    unique = train_new[column_name].unique()\n",
    "    combination = list(combinations(unique, 2))\n",
    "\n",
    "    for i in combination:\n",
    "        A = train_new[train_new[column_name] == i[0]][target]\n",
    "        B = train_new[train_new[column_name] == i[1]][target]\n",
    "\n",
    "        pvalue = ttest_ind(A, B).pvalue\n",
    "\n",
    "        if pvalue <= alfa/len(combination): \n",
    "            print('Есть статистическая значимость для колонки ', target, ' в колонке ', column_name)\n",
    "            list_name.append(column_name)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fitted-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_position\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_hiring_type\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_payment_type\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_passport\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_is_nda_signed\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_is_labor_contract_signed\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_is_added_to_internal_chats\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  assignee_is_added_one_to_one\n",
      "Есть статистическая значимость для колонки  overall_worklogs  в колонке  creator_position\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3723: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\python\\python38\\lib\\site-packages\\numpy\\core\\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "feats_first = []\n",
    "\n",
    "for i in columns_train:\n",
    "    ttest_all(i, 0.01, 'overall_worklogs', feats_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-store",
   "metadata": {},
   "source": [
    "Предобработаем описания задач для дальнейшего получения их векторного представления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distributed-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = re.compile(r'\\W+')\n",
    "digits = re.compile(r'\\d+')\n",
    "probel = re.compile(r'\\s+')\n",
    "\n",
    "train_new['summary'] = train_new['summary'].apply(lambda x: re.sub(digits, ' ', x))\n",
    "\n",
    "train_new['summary'] = train_new['summary'].apply(lambda x: re.sub(punct, ' ', x))\n",
    "\n",
    "train_new['summary'] = train_new['summary'].apply(lambda x: re.sub(probel, ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "optical-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "sents = train_new.summary.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-composer",
   "metadata": {},
   "source": [
    "Посмотрим максимальную длину описания, чтобы ничего не упустить при передаче в машину для получения вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "blocked-bundle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX sentence len:  68\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for sent in sents:\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    \n",
    "print ('MAX sentence len: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-azerbaijan",
   "metadata": {},
   "source": [
    "Передадим данные в машину и получим векторное представление описания задач"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "experimental-hollow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "inpt2 = [tokenizer.encode(sent, max_length=69, pad_to_max_length=True, return_tensors='pt') for sent in sents]\n",
    "model_bert_cls = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def getPoolerOut(model, inpt3, batch_size=32):\n",
    "    model.eval()\n",
    "    \n",
    "    pooler_output = torch.empty([0, 768])\n",
    "    \n",
    "          \n",
    "    for b in range(len(inpt3)//batch_size + 1):\n",
    "        print(b)\n",
    "        up_to = b*batch_size + batch_size\n",
    "        if len(inpt3) < up_to:\n",
    "            up_to = len(inpt3)\n",
    "        input_ids = torch.cat(inpt3[b*batch_size:up_to])\n",
    "          \n",
    "        with torch.no_grad():\n",
    "            embedding = model.forward(input_ids=input_ids, output_hidden_states=True)['hidden_states'][-1][:,0]\n",
    "            pooler_output = torch.cat([pooler_output, embedding],0)\n",
    "            \n",
    "          \n",
    "    return pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "saved-riding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "vectors = getPoolerOut(model_bert_cls, inpt2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "visible-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list = vectors.tolist()\n",
    "vec_df = pd.DataFrame(vec_list)\n",
    "vec_df = pd.DataFrame(vec_df[[0]])\n",
    "vec_df = vec_df.rename(columns={0 : 'CLS_1'})\n",
    "\n",
    "train_new = pd.merge(train_new, vec_df, on = train_new.index, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "turkish-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_first.append('CLS_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-celebration",
   "metadata": {},
   "source": [
    "Инициализируем машину, обучим ее и провалидируем результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "boring-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_overall = LGBMRegressor(boosting_type = 'gbdt', max_depth=5, n_estimators=225, learning_rate=0.05, \n",
    "                                     num_leaves = 45, metric='r2', loss='mae', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "optimum-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_new[feats_first]\n",
    "y_train = train_new['overall_worklogs']\n",
    "\n",
    "x_train_t, x_val, y_train_t, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bizarre-vessel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6588e4c8494e7c9193333b37528001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in trange(50):\n",
    "    model_overall.fit(x_train_t, y_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "instructional-footwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fd4357ede346a1b370bb572e3ff365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in trange(50):\n",
    "    pred = model_overall.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "metric-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = pd.DataFrame(pred)\n",
    "pred2[0] = pred2[0].apply(lambda x: round(x, 0))\n",
    "p = pred2[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unnecessary-track",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2741481994767059"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_val, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-looking",
   "metadata": {},
   "source": [
    "Так как бралась случайная выборка для валидации, то в ней возможны выбросы и потому коэффициент детерминации низкий, при этом то, что не является выбросами, предсказывается точно.\n",
    "\n",
    "Повторим все шаги для получения предсказаний на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "incident-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iss_empl_assignie = pd.merge(test_issues, employees_assignee, on = 'assignee_id', how = 'left')\n",
    "test_new = pd.merge(test_iss_empl_assignie, employees_creator, on = 'creator_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accredited-decline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "sents_test = test_new.summary.values\n",
    "inpt2 = [tokenizer.encode(sent, max_length=69, pad_to_max_length=True, return_tensors='pt') for sent in sents_test]\n",
    "vectors_test = getPoolerOut(model_bert_cls, inpt2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "conventional-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list_test = vectors_test.tolist()\n",
    "vec_df_test = pd.DataFrame(vec_list_test)\n",
    "vec_df_test = pd.DataFrame(vec_df_test[[0]])\n",
    "vec_df_test = vec_df_test.rename(columns={0 : 'CLS_1'})\n",
    "test_new = pd.merge(test_new, vec_df_test, on = test_new.index, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "floating-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat = test_new[feats_first]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "vietnamese-superintendent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d7bff1d65f4dbbb27c3a018ad2542a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in trange(50):\n",
    "    test_pred = model_overall.predict(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "interesting-while",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>overall_worklogs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>675975</td>\n",
       "      <td>11409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>675972</td>\n",
       "      <td>12569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>675965</td>\n",
       "      <td>12014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>675961</td>\n",
       "      <td>12957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>675955</td>\n",
       "      <td>17173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>702545</td>\n",
       "      <td>13995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>702528</td>\n",
       "      <td>15433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>702499</td>\n",
       "      <td>12637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>702376</td>\n",
       "      <td>13995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>670929</td>\n",
       "      <td>13788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  overall_worklogs\n",
       "0     675975             11409\n",
       "1     675972             12569\n",
       "2     675965             12014\n",
       "3     675961             12957\n",
       "4     675955             17173\n",
       "...      ...               ...\n",
       "1065  702545             13995\n",
       "1066  702528             15433\n",
       "1067  702499             12637\n",
       "1068  702376             13995\n",
       "1069  670929             13788\n",
       "\n",
       "[1070 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id = test_new['id']\n",
    "test_pred_df = pd.DataFrame(test_pred)\n",
    "test_pred_df[0] = test_pred_df[0].apply(lambda x: round(x))\n",
    "test_pred_df = test_pred_df.rename(columns={0 : 'overall_worklogs'})\n",
    "shu_sub = pd.concat([test_id, test_pred_df], axis=1)\n",
    "shu_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-treaty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
